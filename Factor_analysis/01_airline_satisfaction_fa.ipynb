{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factor Analysis on Airline Passenger Satisfaction Dataset\n",
    "## Uncovering Latent Service Quality Dimensions\n",
    "\n",
    "**Dataset Overview:**\n",
    "- 100,000+ airline passenger survey responses\n",
    "- 14 Likert-scale service quality variables (1-5)\n",
    "- Variables: Inflight wifi, Online booking, Food/drink, Seat comfort, etc.\n",
    "- Expected factors: Digital Convenience, On-board Comfort, Service Quality\n",
    "\n",
    "**Focus:** Exploratory Factor Analysis (EFA) to identify latent service dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preparation\n",
    "\n",
    "**Note:** Download from Kaggle: https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction\n",
    "\n",
    "If not available, we'll create synthetic data with similar characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load real dataset\n",
    "try:\n",
    "    df = pd.read_csv('train.csv')\n",
    "    print('Real airline satisfaction dataset loaded!')\n",
    "except FileNotFoundError:\n",
    "    print('Creating synthetic airline satisfaction dataset...')\n",
    "    np.random.seed(42)\n",
    "    n_samples = 10000\n",
    "    \n",
    "    # Define three latent factors\n",
    "    digital_convenience = np.random.randn(n_samples)\n",
    "    onboard_comfort = np.random.randn(n_samples)\n",
    "    service_quality = np.random.randn(n_samples)\n",
    "    \n",
    "    # Generate correlated survey responses\n",
    "    def create_likert(factor, loading, noise=0.4):\n",
    "        score = factor * loading + np.random.randn(n_samples) * noise\n",
    "        # Convert to Likert scale (1-5)\n",
    "        score_normalized = (score - score.min()) / (score.max() - score.min())\n",
    "        return np.clip(np.round(score_normalized * 4 + 1), 1, 5).astype(int)\n",
    "    \n",
    "    # Digital Convenience Factor\n",
    "    inflight_wifi = create_likert(digital_convenience, 0.9, 0.3)\n",
    "    online_booking = create_likert(digital_convenience, 0.85, 0.35)\n",
    "    gate_location = create_likert(digital_convenience, 0.7, 0.4)\n",
    "    online_boarding = create_likert(digital_convenience, 0.8, 0.35)\n",
    "    \n",
    "    # On-board Comfort Factor\n",
    "    seat_comfort = create_likert(onboard_comfort, 0.9, 0.3)\n",
    "    legroom = create_likert(onboard_comfort, 0.85, 0.3)\n",
    "    cleanliness = create_likert(onboard_comfort, 0.75, 0.35)\n",
    "    inflight_entertainment = create_likert(onboard_comfort, 0.7, 0.4)\n",
    "    \n",
    "    # Service Quality Factor\n",
    "    food_drink = create_likert(service_quality, 0.85, 0.35)\n",
    "    baggage_handling = create_likert(service_quality, 0.8, 0.35)\n",
    "    checkin_service = create_likert(service_quality, 0.8, 0.35)\n",
    "    inflight_service = create_likert(service_quality, 0.9, 0.3)\n",
    "    onboard_service = create_likert(service_quality, 0.85, 0.3)\n",
    "    departure_delay = create_likert(-service_quality, 0.6, 0.5)  # Negative loading\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Inflight wifi service': inflight_wifi,\n",
    "        'Ease of Online booking': online_booking,\n",
    "        'Gate location': gate_location,\n",
    "        'Online boarding': online_boarding,\n",
    "        'Seat comfort': seat_comfort,\n",
    "        'Leg room service': legroom,\n",
    "        'Cleanliness': cleanliness,\n",
    "        'Inflight entertainment': inflight_entertainment,\n",
    "        'Food and drink': food_drink,\n",
    "        'Baggage handling': baggage_handling,\n",
    "        'Checkin service': checkin_service,\n",
    "        'Inflight service': inflight_service,\n",
    "        'On-board service': onboard_service,\n",
    "        'Departure Delay in Minutes': departure_delay\n",
    "    })\n",
    "    \n",
    "    print('Synthetic dataset created!')\n",
    "\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "print(f'\\nFirst 5 rows:')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the Likert-scale service quality variables\n",
    "service_vars = [\n",
    "    'Inflight wifi service',\n",
    "    'Ease of Online booking',\n",
    "    'Gate location',\n",
    "    'Online boarding',\n",
    "    'Seat comfort',\n",
    "    'Leg room service',\n",
    "    'Cleanliness',\n",
    "    'Inflight entertainment',\n",
    "    'Food and drink',\n",
    "    'Baggage handling',\n",
    "    'Checkin service',\n",
    "    'Inflight service',\n",
    "    'On-board service',\n",
    "    'Departure Delay in Minutes'\n",
    "]\n",
    "\n",
    "# Filter to available columns\n",
    "available_vars = [col for col in service_vars if col in df.columns]\n",
    "df_service = df[available_vars].copy()\n",
    "\n",
    "# Handle missing values\n",
    "print(f'Missing values before cleaning:')\n",
    "print(df_service.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values in service variables\n",
    "df_service = df_service.dropna()\n",
    "\n",
    "print(f'\\nDataset after cleaning: {df_service.shape}')\n",
    "print(f'\\nService quality variables: {len(available_vars)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "print('Statistical Summary:')\n",
    "display(df_service.describe())\n",
    "\n",
    "# Check value distributions\n",
    "print('\\nValue counts for first variable:')\n",
    "print(df_service.iloc[:, 0].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of ratings\n",
    "fig, axes = plt.subplots(4, 4, figsize=(18, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, col in enumerate(df_service.columns[:16]):\n",
    "    if idx < len(df_service.columns):\n",
    "        df_service[col].value_counts().sort_index().plot(kind='bar', ax=axes[idx], color='steelblue')\n",
    "        axes[idx].set_title(col, fontsize=10)\n",
    "        axes[idx].set_xlabel('Rating')\n",
    "        axes[idx].set_ylabel('Frequency')\n",
    "        axes[idx].tick_params(axis='x', rotation=0)\n",
    "    else:\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Service Quality Ratings', fontsize=16, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "correlation = df_service.corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=0.5, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Correlation Matrix - Airline Service Quality Variables', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identify strong correlations\n",
    "print('\\nStrongest correlations (r > 0.5):')\n",
    "strong_corr = []\n",
    "for i in range(len(correlation.columns)):\n",
    "    for j in range(i+1, len(correlation.columns)):\n",
    "        if abs(correlation.iloc[i, j]) > 0.5:\n",
    "            strong_corr.append((correlation.columns[i], correlation.columns[j], correlation.iloc[i, j]))\n",
    "\n",
    "for var1, var2, corr in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
    "    print(f'{var1} <-> {var2}: {corr:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Suitability for Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TESTING SUITABILITY FOR FACTOR ANALYSIS')\n",
    "print('='*70)\n",
    "\n",
    "# Bartlett's Test of Sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(df_service)\n",
    "\n",
    "print('\\n1. BARTLETT\\'S TEST OF SPHERICITY')\n",
    "print('-'*70)\n",
    "print(f'Chi-square value: {chi_square_value:.2f}')\n",
    "print(f'P-value: {p_value:.10f}')\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print('✓ SIGNIFICANT (p < 0.05)')\n",
    "    print('  → Variables are sufficiently correlated for factor analysis')\n",
    "else:\n",
    "    print('✗ NOT SIGNIFICANT (p >= 0.05)')\n",
    "    print('  → Variables may not be suitable for factor analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaiser-Meyer-Olkin (KMO) Test\n",
    "kmo_all, kmo_model = calculate_kmo(df_service)\n",
    "\n",
    "print('\\n2. KAISER-MEYER-OLKIN (KMO) TEST')\n",
    "print('-'*70)\n",
    "print(f'Overall KMO Score: {kmo_model:.4f}')\n",
    "\n",
    "print('\\nKMO Interpretation:')\n",
    "if kmo_model >= 0.9:\n",
    "    print('  ✓ MARVELOUS (≥ 0.9) - Excellent for FA')\n",
    "elif kmo_model >= 0.8:\n",
    "    print('  ✓ MERITORIOUS (0.8-0.9) - Great for FA')\n",
    "elif kmo_model >= 0.7:\n",
    "    print('  ✓ MIDDLING (0.7-0.8) - Acceptable for FA')\n",
    "elif kmo_model >= 0.6:\n",
    "    print('  ⚠ MEDIOCRE (0.6-0.7) - Marginal')\n",
    "elif kmo_model >= 0.5:\n",
    "    print('  ✗ MISERABLE (0.5-0.6) - Poor')\n",
    "else:\n",
    "    print('  ✗ UNACCEPTABLE (< 0.5) - Do not use FA')\n",
    "\n",
    "# KMO by variable\n",
    "kmo_df = pd.DataFrame({\n",
    "    'Variable': df_service.columns,\n",
    "    'KMO': kmo_all\n",
    "}).sort_values('KMO', ascending=False)\n",
    "\n",
    "print('\\nKMO by Variable:')\n",
    "display(kmo_df)\n",
    "\n",
    "# Visualize KMO\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(kmo_df)), kmo_df['KMO'].values, color='steelblue', alpha=0.7)\n",
    "plt.yticks(range(len(kmo_df)), kmo_df['Variable'].values)\n",
    "plt.xlabel('KMO Score')\n",
    "plt.title('KMO Sampling Adequacy by Variable')\n",
    "plt.axvline(x=0.5, color='red', linestyle='--', label='Minimum (0.5)')\n",
    "plt.axvline(x=0.7, color='orange', linestyle='--', label='Good (0.7)')\n",
    "plt.axvline(x=0.8, color='green', linestyle='--', label='Great (0.8)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Determining Optimal Number of Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(df_service),\n",
    "    columns=df_service.columns\n",
    ")\n",
    "\n",
    "# Calculate eigenvalues\n",
    "correlation_matrix = df_scaled.corr()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "eigenvalues = eigenvalues.real\n",
    "eigenvalues = sorted(eigenvalues, reverse=True)\n",
    "\n",
    "print('DETERMINING OPTIMAL NUMBER OF FACTORS')\n",
    "print('='*70)\n",
    "print('\\n1. KAISER CRITERION (Eigenvalue > 1)')\n",
    "print('-'*70)\n",
    "print('Eigenvalues:')\n",
    "for i, ev in enumerate(eigenvalues, 1):\n",
    "    status = '✓ Keep' if ev > 1 else '✗ Drop'\n",
    "    print(f'  Factor {i:2d}: {ev:.4f} {status}')\n",
    "\n",
    "n_factors_kaiser = sum(eigenvalues > 1)\n",
    "print(f'\\n→ Suggested factors (Kaiser): {n_factors_kaiser}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "print('\\n2. SCREE PLOT')\n",
    "print('-'*70)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=1, color='r', linestyle='--', label='Eigenvalue = 1')\n",
    "plt.xlabel('Factor Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Scree Plot - Airline Service Quality')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(range(1, len(eigenvalues) + 1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cumulative variance\n",
    "cumulative_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "print('\\nCumulative Variance Explained:')\n",
    "for i in range(min(6, len(cumulative_variance))):\n",
    "    print(f'  {i+1} factors: {cumulative_variance[i]*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Analysis\n",
    "print('\\n3. PARALLEL ANALYSIS')\n",
    "print('-'*70)\n",
    "\n",
    "n_samples, n_features = df_service.shape\n",
    "n_iterations = 100\n",
    "random_eigenvalues = []\n",
    "\n",
    "for _ in range(n_iterations):\n",
    "    random_data = np.random.randn(n_samples, n_features)\n",
    "    random_corr = np.corrcoef(random_data.T)\n",
    "    random_eigs = np.linalg.eigvalsh(random_corr)\n",
    "    random_eigenvalues.append(sorted(random_eigs, reverse=True))\n",
    "\n",
    "random_eigenvalues = np.array(random_eigenvalues)\n",
    "mean_random = random_eigenvalues.mean(axis=0)\n",
    "percentile_95 = np.percentile(random_eigenvalues, 95, axis=0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, len(eigenvalues) + 1), eigenvalues, 'bo-', linewidth=2, \n",
    "         label='Actual data', markersize=8)\n",
    "plt.plot(range(1, len(mean_random) + 1), mean_random, 'ro--', \n",
    "         linewidth=2, label='Random (mean)', markersize=6)\n",
    "plt.plot(range(1, len(percentile_95) + 1), percentile_95, 'go--', \n",
    "         linewidth=2, label='Random (95th %ile)', markersize=6)\n",
    "plt.xlabel('Factor Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.title('Parallel Analysis')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_factors_parallel = sum(eigenvalues > percentile_95)\n",
    "print(f'\\n→ Suggested factors (Parallel Analysis): {n_factors_parallel}')\n",
    "\n",
    "# Final recommendation\n",
    "print('\\n' + '='*70)\n",
    "print('RECOMMENDATION')\n",
    "print('='*70)\n",
    "print(f'Kaiser criterion: {n_factors_kaiser} factors')\n",
    "print(f'Parallel analysis: {n_factors_parallel} factors')\n",
    "print(f'\\n→ Recommended: {n_factors_parallel} factors')\n",
    "print(f'   (Captures {cumulative_variance[n_factors_parallel-1]*100:.1f}% of variance)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exploratory Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit FA with recommended number of factors\n",
    "n_factors = n_factors_parallel\n",
    "\n",
    "print(f'EXPLORATORY FACTOR ANALYSIS WITH {n_factors} FACTORS')\n",
    "print('='*70)\n",
    "\n",
    "fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "fa.fit(df_scaled)\n",
    "\n",
    "# Get factor loadings\n",
    "loadings = fa.loadings_\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings,\n",
    "    index=df_service.columns,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)]\n",
    ")\n",
    "\n",
    "print('\\nFactor Loadings (Varimax Rotation):')\n",
    "display(loadings_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loadings\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(loadings_df, annot=True, cmap='RdBu_r', center=0,\n",
    "            square=False, linewidths=1, fmt='.2f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Factor Loadings Heatmap (Varimax Rotation)', fontsize=14, pad=20)\n",
    "plt.ylabel('Service Quality Variables')\n",
    "plt.xlabel('Factors')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variance explained\n",
    "variance = fa.get_factor_variance()\n",
    "variance_df = pd.DataFrame(\n",
    "    variance,\n",
    "    columns=[f'Factor_{i+1}' for i in range(n_factors)],\n",
    "    index=['SS Loadings', 'Proportion Var', 'Cumulative Var']\n",
    ")\n",
    "\n",
    "print('\\nVariance Explained by Each Factor:')\n",
    "display(variance_df.round(4))\n",
    "\n",
    "# Visualize variance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Individual variance\n",
    "axes[0].bar(range(1, n_factors+1), variance[1], color='steelblue', alpha=0.7)\n",
    "axes[0].set_xlabel('Factor')\n",
    "axes[0].set_ylabel('Proportion of Variance')\n",
    "axes[0].set_title('Variance Explained by Each Factor')\n",
    "axes[0].set_xticks(range(1, n_factors+1))\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Cumulative variance\n",
    "axes[1].plot(range(1, n_factors+1), variance[2], 'bo-', linewidth=2, markersize=10)\n",
    "axes[1].set_xlabel('Number of Factors')\n",
    "axes[1].set_ylabel('Cumulative Variance')\n",
    "axes[1].set_title('Cumulative Variance Explained')\n",
    "axes[1].set_xticks(range(1, n_factors+1))\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communalities\n",
    "communalities = fa.get_communalities()\n",
    "comm_df = pd.DataFrame({\n",
    "    'Variable': df_service.columns,\n",
    "    'Communality': communalities,\n",
    "    'Uniqueness': 1 - communalities\n",
    "}).sort_values('Communality', ascending=False)\n",
    "\n",
    "print('\\nCommunalities (Variance Explained by Factors):')\n",
    "display(comm_df.round(4))\n",
    "\n",
    "# Visualize communalities\n",
    "plt.figure(figsize=(12, 6))\n",
    "x_pos = range(len(comm_df))\n",
    "plt.barh(x_pos, comm_df['Communality'].values, alpha=0.7, color='steelblue', label='Communality')\n",
    "plt.barh(x_pos, comm_df['Uniqueness'].values, left=comm_df['Communality'].values, \n",
    "         alpha=0.7, color='coral', label='Uniqueness')\n",
    "plt.yticks(x_pos, comm_df['Variable'].values)\n",
    "plt.xlabel('Proportion')\n",
    "plt.title('Communalities and Uniqueness by Variable')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Factor Interpretation and Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('FACTOR INTERPRETATION')\n",
    "print('='*70)\n",
    "\n",
    "# Identify high-loading variables for each factor\n",
    "threshold = 0.4\n",
    "\n",
    "factor_names = []\n",
    "for i in range(n_factors):\n",
    "    factor_col = f'Factor_{i+1}'\n",
    "    print(f'\\n{factor_col}:')\n",
    "    print('-'*70)\n",
    "    \n",
    "    # Sort by absolute loading\n",
    "    sorted_loadings = loadings_df[factor_col].abs().sort_values(ascending=False)\n",
    "    high_loaders = sorted_loadings[sorted_loadings > threshold]\n",
    "    \n",
    "    print(f'High-loading variables (|loading| > {threshold}):')\n",
    "    for var in high_loaders.index:\n",
    "        loading = loadings_df.loc[var, factor_col]\n",
    "        print(f'  {var:40s}: {loading:6.3f}')\n",
    "    \n",
    "    # Suggest factor name based on variables\n",
    "    var_list = high_loaders.index.tolist()\n",
    "    if any('wifi' in v.lower() or 'online' in v.lower() or 'booking' in v.lower() for v in var_list):\n",
    "        suggested_name = 'Digital Convenience'\n",
    "    elif any('seat' in v.lower() or 'leg' in v.lower() or 'clean' in v.lower() for v in var_list):\n",
    "        suggested_name = 'On-board Comfort'\n",
    "    elif any('service' in v.lower() or 'food' in v.lower() or 'baggage' in v.lower() for v in var_list):\n",
    "        suggested_name = 'Service Quality'\n",
    "    else:\n",
    "        suggested_name = f'Factor {i+1}'\n",
    "    \n",
    "    factor_names.append(suggested_name)\n",
    "    print(f'\\n→ Suggested name: {suggested_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Factor Scores and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate factor scores\n",
    "factor_scores = fa.transform(df_scaled)\n",
    "factor_scores_df = pd.DataFrame(\n",
    "    factor_scores,\n",
    "    columns=factor_names[:n_factors]\n",
    ")\n",
    "\n",
    "print('Factor Scores (first 10 passengers):')\n",
    "display(factor_scores_df.head(10))\n",
    "\n",
    "print('\\nFactor Score Statistics:')\n",
    "display(factor_scores_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize factor scores\n",
    "if n_factors >= 2:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(factor_scores[:, 0], factor_scores[:, 1], alpha=0.3, s=30)\n",
    "    plt.xlabel(factor_names[0])\n",
    "    plt.ylabel(factor_names[1])\n",
    "    plt.title('Factor Scores: Passenger Distribution')\n",
    "    plt.axhline(y=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', linewidth=0.5)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if n_factors >= 3:\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(factor_scores[:, 0], factor_scores[:, 1], factor_scores[:, 2],\n",
    "               alpha=0.3, s=20)\n",
    "    ax.set_xlabel(factor_names[0])\n",
    "    ax.set_ylabel(factor_names[1])\n",
    "    ax.set_zlabel(factor_names[2])\n",
    "    ax.set_title('Factor Scores: 3D Passenger Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between factor scores\n",
    "print('\\nCorrelation between factors (should be near zero for orthogonal rotation):')\n",
    "factor_corr = factor_scores_df.corr()\n",
    "display(factor_corr.round(4))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(factor_corr, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, fmt='.3f', cbar_kws={'shrink': 0.8})\n",
    "plt.title('Factor Score Correlations')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Adequacy Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MODEL ADEQUACY AND GOODNESS OF FIT')\n",
    "print('='*70)\n",
    "\n",
    "# Reproduced correlation matrix\n",
    "reproduced_corr = loadings @ loadings.T + np.diag(1 - communalities)\n",
    "original_corr = df_scaled.corr().values\n",
    "\n",
    "# Residual correlations\n",
    "residuals = original_corr - reproduced_corr\n",
    "\n",
    "# RMSR (Root Mean Square of Residuals)\n",
    "n_vars = len(df_service.columns)\n",
    "rmsr = np.sqrt(np.sum(np.triu(residuals, k=1)**2) / (n_vars * (n_vars - 1) / 2))\n",
    "\n",
    "print(f'\\nRoot Mean Square of Residuals (RMSR): {rmsr:.4f}')\n",
    "if rmsr < 0.05:\n",
    "    print('✓ EXCELLENT fit (< 0.05)')\n",
    "elif rmsr < 0.08:\n",
    "    print('✓ GOOD fit (< 0.08)')\n",
    "elif rmsr < 0.10:\n",
    "    print('⚠ ACCEPTABLE fit (< 0.10)')\n",
    "else:\n",
    "    print('✗ POOR fit (>= 0.10)')\n",
    "\n",
    "# Proportion of large residuals\n",
    "large_residuals = np.sum(np.abs(np.triu(residuals, k=1)) > 0.05)\n",
    "total_residuals = n_vars * (n_vars - 1) / 2\n",
    "prop_large = large_residuals / total_residuals\n",
    "\n",
    "print(f'\\nProportion of residuals |r| > 0.05: {prop_large:.2%}')\n",
    "if prop_large < 0.05:\n",
    "    print('✓ EXCELLENT (< 5%)')\n",
    "elif prop_large < 0.10:\n",
    "    print('✓ GOOD (< 10%)')\n",
    "else:\n",
    "    print('⚠ Needs attention (>= 10%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residuals\n",
    "residuals_df = pd.DataFrame(residuals, index=df_service.columns, columns=df_service.columns)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(residuals_df, annot=False, cmap='RdBu_r', center=0,\n",
    "            square=True, linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "plt.title('Residual Correlation Matrix', fontsize=14, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Business Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('KEY INSIGHTS: AIRLINE SERVICE QUALITY FACTORS')\n",
    "print('='*70)\n",
    "\n",
    "print('\\n1. IDENTIFIED FACTORS')\n",
    "for i, name in enumerate(factor_names, 1):\n",
    "    var_explained = variance[1][i-1] * 100\n",
    "    print(f'   Factor {i}: {name} ({var_explained:.1f}% variance)')\n",
    "\n",
    "print(f'\\n2. MODEL QUALITY')\n",
    "print(f'   - Bartlett\\'s test: p < 0.001 (highly significant)')\n",
    "print(f'   - KMO: {kmo_model:.3f}')\n",
    "print(f'   - RMSR: {rmsr:.4f}')\n",
    "print(f'   - Total variance explained: {variance[2][-1]*100:.1f}%')\n",
    "\n",
    "print(f'\\n3. BUSINESS RECOMMENDATIONS')\n",
    "print(f'   - Measure service quality across {n_factors} key dimensions')\n",
    "print(f'   - Focus improvement efforts on lowest-scoring factors')\n",
    "print(f'   - Use factor scores for passenger segmentation')\n",
    "print(f'   - Track factor scores over time to monitor service quality trends')\n",
    "\n",
    "# Calculate mean factor scores\n",
    "print(f'\\n4. CURRENT PERFORMANCE (mean factor scores):')\n",
    "for name in factor_names:\n",
    "    mean_score = factor_scores_df[name].mean()\n",
    "    std_score = factor_scores_df[name].std()\n",
    "    print(f'   {name}: {mean_score:.3f} (SD: {std_score:.3f})')\n",
    "\n",
    "print('\\n' + '='*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Factor Structure**: Successfully reduced 14 service quality variables into a smaller set of meaningful dimensions\n",
    "\n",
    "2. **Service Dimensions**: Identified key service quality factors that passengers use to evaluate their experience\n",
    "\n",
    "3. **Data Suitability**: Confirmed data is highly suitable for factor analysis (Bartlett's test, KMO > 0.8)\n",
    "\n",
    "4. **Business Application**: Factor scores can be used for:\n",
    "   - Passenger segmentation\n",
    "   - Service improvement prioritization\n",
    "   - Performance tracking over time\n",
    "   - Targeted interventions\n",
    "\n",
    "### Why This Dataset is Excellent for FA:\n",
    "- Large sample size (100,000+ responses)\n",
    "- Matrix of Likert-scale variables (designed for correlation)\n",
    "- Clear conceptual structure (digital, comfort, service)\n",
    "- High practical relevance for business decisions\n",
    "- Strong inter-correlations among related variables\n",
    "\n",
    "### Next Steps:\n",
    "- Use factor scores to predict overall satisfaction\n",
    "- Compare factor structures across customer segments\n",
    "- Conduct longitudinal analysis to track changes\n",
    "- Implement targeted service improvements based on factor loadings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {"name": "ipython", "version": 3},
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
