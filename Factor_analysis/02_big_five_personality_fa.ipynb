{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Factor Analysis on Big Five Personality Test Dataset\n## Confirmatory Factor Analysis of the Five-Factor Model\n\n**Dataset Overview:**\n- 50 personality questions based on IPIP-FFM\n- Expected 5 factors: Extroversion, Neuroticism, Agreeableness, Conscientiousness, Openness\n- Large sample size (often 10,000+ responses)\n- Perfect for Confirmatory Factor Analysis (CFA)\n\n**Focus:** Testing theoretical factor structure and cross-loadings"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom factor_analyzer import FactorAnalyzer, calculate_bartlett_sphericity, calculate_kmo\nfrom sklearn.preprocessing import StandardScaler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 8)\nprint('Libraries imported!')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Data Loading\n\nDownload from: https://www.kaggle.com/datasets/tunguz/big-five-personality-test"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "try:\n    df = pd.read_csv('data.csv', sep='\\t')\n    print('Real Big Five dataset loaded!')\nexcept:\n    print('Creating synthetic Big Five personality data...')\n    np.random.seed(42)\n    n = 5000\n    \n    # Generate 5 latent factors\n    extroversion = np.random.randn(n)\n    neuroticism = np.random.randn(n)\n    agreeableness = np.random.randn(n)\n    conscientiousness = np.random.randn(n)\n    openness = np.random.randn(n)\n    \n    def likert(factor, loading, reverse=False, noise=0.6):\n        score = factor * loading + np.random.randn(n) * noise\n        if reverse:\n            score = -score\n        score = (score - score.min()) / (score.max() - score.min())\n        return np.clip(np.round(score * 4 + 1), 1, 5).astype(int)\n    \n    # Create 50 questions (10 per factor)\n    data = {}\n    \n    # Extroversion (E1-E10)\n    for i in range(1, 11):\n        reverse = i % 2 == 0\n        data[f'E{i}'] = likert(extroversion, 0.8, reverse)\n    \n    # Neuroticism (N1-N10)\n    for i in range(1, 11):\n        reverse = i % 2 == 0\n        data[f'N{i}'] = likert(neuroticism, 0.8, reverse)\n    \n    # Agreeableness (A1-A10)\n    for i in range(1, 11):\n        reverse = i % 2 == 0\n        data[f'A{i}'] = likert(agreeableness, 0.8, reverse)\n    \n    # Conscientiousness (C1-C10)\n    for i in range(1, 11):\n        reverse = i % 2 == 0\n        data[f'C{i}'] = likert(conscientiousness, 0.8, reverse)\n    \n    # Openness (O1-O10)\n    for i in range(1, 11):\n        reverse = i % 2 == 0\n        data[f'O{i}'] = likert(openness, 0.8, reverse)\n    \n    df = pd.DataFrame(data)\n    print('Synthetic dataset created!')\n\nprint(f'\\nShape: {df.shape}')\nprint(f'\\nFirst 5 rows:')\ndisplay(df.head())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Exploration and Preprocessing"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Handle reverse-scored items\nreverse_items = [col for col in df.columns if int(col[1:]) % 2 == 0]\n\nprint(f'Reverse-scored items: {len(reverse_items)}')\nfor item in reverse_items[:5]:\n    print(f'  {item}: before reverse - mean={df[item].mean():.2f}')\n\n# Reverse scoring (6 - value for 1-5 scale)\nfor item in reverse_items:\n    df[item] = 6 - df[item]\n\nprint('\\nAfter reversing:')\nfor item in reverse_items[:5]:\n    print(f'  {item}: after reverse - mean={df[item].mean():.2f}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Calculate scale scores\nextroversion_cols = [c for c in df.columns if c.startswith('E')]\nneuroticism_cols = [c for c in df.columns if c.startswith('N')]\nagreeableness_cols = [c for c in df.columns if c.startswith('A')]\nconscientiousness_cols = [c for c in df.columns if c.startswith('C')]\nopenness_cols = [c for c in df.columns if c.startswith('O')]\n\ndf['Extroversion_Score'] = df[extroversion_cols].mean(axis=1)\ndf['Neuroticism_Score'] = df[neuroticism_cols].mean(axis=1)\ndf['Agreeableness_Score'] = df[agreeableness_cols].mean(axis=1)\ndf['Conscientiousness_Score'] = df[conscientiousness_cols].mean(axis=1)\ndf['Openness_Score'] = df[openness_cols].mean(axis=1)\n\nprint('Big Five Scores:')\nscores = df[['Extroversion_Score', 'Neuroticism_Score', 'Agreeableness_Score',\n             'Conscientiousness_Score', 'Openness_Score']]\ndisplay(scores.describe())"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Suitability Tests"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Select only the 50 personality items\nitems_df = df[[c for c in df.columns if len(c) <= 3]]\n\nchi2, p = calculate_bartlett_sphericity(items_df)\nkmo_all, kmo_model = calculate_kmo(items_df)\n\nprint('SUITABILITY TESTS')\nprint('='*70)\nprint(f'\\nBartlett\\'s Test: Chi2={chi2:.2f}, p={p:.10f}')\nprint(f'KMO: {kmo_model:.4f}')\n\nif p < 0.05 and kmo_model > 0.6:\n    print('\\n\u2713 Data is suitable for factor analysis')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. Confirmatory Factor Analysis with 5 Factors"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Standardize\nscaler = StandardScaler()\nitems_scaled = pd.DataFrame(scaler.fit_transform(items_df), columns=items_df.columns)\n\n# Fit 5-factor model\nfa = FactorAnalyzer(n_factors=5, rotation='varimax')\nfa.fit(items_scaled)\n\nloadings = fa.loadings_\nloadings_df = pd.DataFrame(loadings, index=items_df.columns,\n                          columns=['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5'])\n\nprint('Factor Loadings (5-Factor Solution):')\ndisplay(loadings_df.round(3))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Identify which items load on which factor\nfor i in range(5):\n    factor_col = f'Factor_{i+1}'\n    print(f'\\n{factor_col}:')\n    high_loaders = loadings_df[factor_col].abs().sort_values(ascending=False).head(10)\n    for item, loading in high_loaders.items():\n        original_loading = loadings_df.loc[item, factor_col]\n        print(f'  {item}: {original_loading:.3f}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Check for cross-loadings\nprint('\\nCROSS-LOADINGS (items loading >0.3 on multiple factors):')\nfor item in loadings_df.index:\n    loadings_abs = loadings_df.loc[item].abs()\n    high_loadings = loadings_abs[loadings_abs > 0.3]\n    if len(high_loadings) > 1:\n        print(f'{item}: {high_loadings.to_dict()}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Variance Explained"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "variance = fa.get_factor_variance()\nvariance_df = pd.DataFrame(variance, \n                           columns=['Factor_1', 'Factor_2', 'Factor_3', 'Factor_4', 'Factor_5'],\n                           index=['SS Loadings', 'Proportion Var', 'Cumulative Var'])\n\nprint('Variance Explained:')\ndisplay(variance_df.round(4))\n\nplt.figure(figsize=(12, 6))\nplt.bar(range(1, 6), variance[1], color='steelblue', alpha=0.7)\nplt.xlabel('Factor')\nplt.ylabel('Proportion of Variance')\nplt.title('Variance Explained by Big Five Factors')\nplt.xticks(range(1, 6))\nplt.grid(True, alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Model Fit Assessment"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "communalities = fa.get_communalities()\ncomm_df = pd.DataFrame({'Item': items_df.columns, 'Communality': communalities,\n                       'Uniqueness': 1-communalities}).sort_values('Communality', ascending=False)\n\nprint('Communalities:')\ndisplay(comm_df.head(20))\n\nprint(f'\\nMean communality: {communalities.mean():.3f}')\nprint(f'Items with low communality (<0.3): {(communalities < 0.3).sum()}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. Summary\n\nThis notebook demonstrated Confirmatory Factor Analysis on the Big Five personality model. The analysis revealed:\n\n1. Strong evidence for 5-factor structure\n2. Items generally load on their intended factors\n3. Some cross-loadings indicate complexity in personality measurement\n4. Model explains substantial variance in personality responses\n\nThe Big Five dataset is ideal for learning FA because:\n- Clear theoretical structure (5 factors)\n- Large sample size\n- Well-validated measurement model\n- Demonstrates both convergent and discriminant validity"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}