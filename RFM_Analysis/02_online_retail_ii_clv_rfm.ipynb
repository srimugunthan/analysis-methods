{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RFM Analysis - Online Retail II Dataset\n",
    "\n",
    "**Dataset**: Online Retail II from UCI Machine Learning Repository\n",
    "\n",
    "**Source**: https://archive.ics.uci.edu/ml/datasets/Online+Retail+II\n",
    "\n",
    "**Description**: Updated version with additional years (2009-2011)\n",
    "\n",
    "**Complexity**: Medium (Advanced with CLV focus)\n",
    "\n",
    "## Focus Areas\n",
    "- Standard RFM Analysis\n",
    "- **Customer Lifetime Value (CLV) Prediction**\n",
    "- Cohort Analysis\n",
    "- Churn Prediction Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download from UCI or use the Excel files\n",
    "# Two sheets: Year 2009-2010 and Year 2010-2011\n",
    "\n",
    "# Option 1: Load from separate sheets\n",
    "df1 = pd.read_excel('online_retail_II.xlsx', sheet_name='Year 2009-2010')\n",
    "df2 = pd.read_excel('online_retail_II.xlsx', sheet_name='Year 2010-2011')\n",
    "df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Option 2: If already combined CSV\n",
    "# df = pd.read_csv('online_retail_II.csv', encoding='ISO-8859-1')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Standardize column names (Online Retail II might have different names)\n",
    "column_mapping = {\n",
    "    'Invoice': 'InvoiceNo',\n",
    "    'StockCode': 'StockCode',\n",
    "    'Description': 'Description',\n",
    "    'Quantity': 'Quantity',\n",
    "    'InvoiceDate': 'InvoiceDate',\n",
    "    'Price': 'UnitPrice',\n",
    "    'Customer ID': 'CustomerID',\n",
    "    'Country': 'Country'\n",
    "}\n",
    "\n",
    "df = df.rename(columns=column_mapping)\n",
    "\n",
    "# Data cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Remove missing CustomerID\n",
    "df_clean = df_clean[df_clean['CustomerID'].notna()]\n",
    "\n",
    "# Remove cancellations\n",
    "df_clean = df_clean[~df_clean['InvoiceNo'].astype(str).str.startswith('C')]\n",
    "\n",
    "# Remove invalid quantities and prices\n",
    "df_clean = df_clean[(df_clean['Quantity'] > 0) & (df_clean['UnitPrice'] > 0)]\n",
    "\n",
    "# Calculate total amount\n",
    "df_clean['TotalAmount'] = df_clean['Quantity'] * df_clean['UnitPrice']\n",
    "\n",
    "# Remove extreme outliers\n",
    "df_clean = df_clean[df_clean['TotalAmount'] <= df_clean['TotalAmount'].quantile(0.999)]\n",
    "\n",
    "# Convert dates\n",
    "df_clean['InvoiceDate'] = pd.to_datetime(df_clean['InvoiceDate'])\n",
    "df_clean['CustomerID'] = df_clean['CustomerID'].astype(int)\n",
    "\n",
    "# Extract time features\n",
    "df_clean['Year'] = df_clean['InvoiceDate'].dt.year\n",
    "df_clean['Month'] = df_clean['InvoiceDate'].dt.month\n",
    "df_clean['YearMonth'] = df_clean['InvoiceDate'].dt.to_period('M')\n",
    "\n",
    "print(f\"Clean dataset: {df_clean.shape}\")\n",
    "print(f\"Customers: {df_clean['CustomerID'].nunique():,}\")\n",
    "print(f\"Date range: {df_clean['InvoiceDate'].min()} to {df_clean['InvoiceDate'].max()}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RFM Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Set analysis date\n",
    "analysis_date = df_clean['InvoiceDate'].max() + timedelta(days=1)\n",
    "print(f\"Analysis date: {analysis_date}\")\n",
    "\n",
    "# Calculate RFM\n",
    "rfm = df_clean.groupby('CustomerID').agg({\n",
    "    'InvoiceDate': lambda x: (analysis_date - x.max()).days,\n",
    "    'InvoiceNo': 'nunique',\n",
    "    'TotalAmount': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "rfm.columns = ['CustomerID', 'Recency', 'Frequency', 'Monetary']\n",
    "\n",
    "# Additional features for CLV\n",
    "# Calculate average order value and purchase interval\n",
    "customer_stats = df_clean.groupby('CustomerID').agg({\n",
    "    'TotalAmount': ['sum', 'mean', 'std'],\n",
    "    'InvoiceDate': ['min', 'max', 'count']\n",
    "})\n",
    "\n",
    "customer_stats.columns = ['_'.join(col).strip() for col in customer_stats.columns.values]\n",
    "customer_stats['customer_lifespan_days'] = (customer_stats['InvoiceDate_max'] - customer_stats['InvoiceDate_min']).dt.days\n",
    "customer_stats['avg_days_between_purchases'] = customer_stats['customer_lifespan_days'] / (customer_stats['InvoiceDate_count'] - 1)\n",
    "customer_stats['avg_days_between_purchases'] = customer_stats['avg_days_between_purchases'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "rfm = rfm.merge(customer_stats[['TotalAmount_mean', 'TotalAmount_std', 'customer_lifespan_days', 'avg_days_between_purchases']], \n",
    "                left_on='CustomerID', right_index=True)\n",
    "\n",
    "rfm.head(10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RFM Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Quintile-based scoring\n",
    "rfm['R_Score'] = pd.qcut(rfm['Recency'], q=5, labels=[5, 4, 3, 2, 1], duplicates='drop').astype(int)\n",
    "rfm['F_Score'] = pd.qcut(rfm['Frequency'].rank(method='first'), q=5, labels=[1, 2, 3, 4, 5], duplicates='drop').astype(int)\n",
    "rfm['M_Score'] = pd.qcut(rfm['Monetary'], q=5, labels=[1, 2, 3, 4, 5], duplicates='drop').astype(int)\n",
    "\n",
    "rfm['RFM_Score'] = rfm['R_Score'].astype(str) + rfm['F_Score'].astype(str) + rfm['M_Score'].astype(str)\n",
    "rfm['RFM_Total'] = rfm['R_Score'] + rfm['F_Score'] + rfm['M_Score']\n",
    "\n",
    "# Segmentation\n",
    "def segment_customers(df):\n",
    "    segments = []\n",
    "    for _, row in df.iterrows():\n",
    "        r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
    "        \n",
    "        if r >= 4 and f >= 4 and m >= 4:\n",
    "            segment = 'Champions'\n",
    "        elif r >= 3 and f >= 4:\n",
    "            segment = 'Loyal Customers'\n",
    "        elif r >= 4 and 2 <= f <= 3:\n",
    "            segment = 'Potential Loyalists'\n",
    "        elif r >= 4 and f <= 2:\n",
    "            segment = 'New Customers'\n",
    "        elif 3 <= r <= 4 and f <= 2:\n",
    "            segment = 'Promising'\n",
    "        elif r >= 3 and f >= 3 and m >= 3:\n",
    "            segment = 'Need Attention'\n",
    "        elif 2 <= r <= 3:\n",
    "            segment = 'About to Sleep'\n",
    "        elif r <= 2 and f >= 4 and m >= 4:\n",
    "            segment = 'At Risk'\n",
    "        elif r <= 1 and f >= 4 and m >= 4:\n",
    "            segment = \"Can't Lose Them\"\n",
    "        elif r <= 2 and f <= 2:\n",
    "            segment = 'Hibernating'\n",
    "        else:\n",
    "            segment = 'Lost'\n",
    "        segments.append(segment)\n",
    "    return segments\n",
    "\n",
    "rfm['Segment'] = segment_customers(rfm)\n",
    "print(\"Segment Distribution:\")\n",
    "print(rfm['Segment'].value_counts())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customer Lifetime Value (CLV) Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Historical CLV (actual)\n",
    "rfm['Historical_CLV'] = rfm['Monetary']\n",
    "\n",
    "# Predicted CLV - Multiple approaches\n",
    "\n",
    "# Approach 1: Simple extrapolation\n",
    "# CLV = Average Order Value × Purchase Frequency × Customer Lifespan\n",
    "avg_customer_lifespan_years = 2  # Assumption\n",
    "rfm['Simple_CLV'] = (rfm['TotalAmount_mean'] * rfm['Frequency'] * \n",
    "                     (365 * avg_customer_lifespan_years / (rfm['Recency'] + 1)))\n",
    "\n",
    "# Approach 2: Time-based extrapolation\n",
    "# Extrapolate based on customer's historical behavior\n",
    "rfm['Purchase_Rate'] = rfm['Frequency'] / (rfm['customer_lifespan_days'] + 1)\n",
    "rfm['Expected_Purchases_Per_Year'] = rfm['Purchase_Rate'] * 365\n",
    "rfm['Time_Based_CLV'] = (rfm['TotalAmount_mean'] * \n",
    "                         rfm['Expected_Purchases_Per_Year'] * \n",
    "                         avg_customer_lifespan_years)\n",
    "\n",
    "# Approach 3: Weighted CLV considering recency\n",
    "# Recent customers get higher weight\n",
    "max_recency = rfm['Recency'].max()\n",
    "rfm['Recency_Weight'] = 1 - (rfm['Recency'] / max_recency)\n",
    "rfm['Weighted_CLV'] = rfm['Time_Based_CLV'] * (0.5 + 0.5 * rfm['Recency_Weight'])\n",
    "\n",
    "# Approach 4: BG/NBD-inspired probability\n",
    "# Probability customer is \"alive\"\n",
    "rfm['Prob_Alive'] = np.exp(-rfm['Recency'] / rfm['avg_days_between_purchases'].replace(0, rfm['Recency'].mean()))\n",
    "rfm['Probabilistic_CLV'] = rfm['Time_Based_CLV'] * rfm['Prob_Alive']\n",
    "\n",
    "print(\"CLV Summary Statistics:\")\n",
    "print(rfm[['Historical_CLV', 'Simple_CLV', 'Time_Based_CLV', 'Weighted_CLV', 'Probabilistic_CLV']].describe())"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Compare CLV approaches\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "clv_columns = ['Simple_CLV', 'Time_Based_CLV', 'Weighted_CLV', 'Probabilistic_CLV']\n",
    "titles = ['Simple CLV', 'Time-Based CLV', 'Weighted CLV', 'Probabilistic CLV']\n",
    "\n",
    "for idx, (col, title) in enumerate(zip(clv_columns, titles)):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Remove extreme outliers for visualization\n",
    "    data = rfm[rfm[col] <= rfm[col].quantile(0.95)][col]\n",
    "    \n",
    "    ax.hist(data, bins=50, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(data.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: £{data.mean():.2f}')\n",
    "    ax.axvline(data.median(), color='green', linestyle='--', linewidth=2, label=f'Median: £{data.median():.2f}')\n",
    "    \n",
    "    ax.set_xlabel('CLV (£)', fontsize=11)\n",
    "    ax.set_ylabel('Number of Customers', fontsize=11)\n",
    "    ax.set_title(f'{title} Distribution', fontsize=13, fontweight='bold')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# CLV by segment\n",
    "clv_by_segment = rfm.groupby('Segment')[['Historical_CLV', 'Simple_CLV', 'Time_Based_CLV', \n",
    "                                          'Weighted_CLV', 'Probabilistic_CLV']].mean().round(2)\n",
    "\n",
    "print(\"Average CLV by Segment:\")\n",
    "clv_by_segment"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize CLV by segment\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Historical vs Predicted CLV\n",
    "clv_comparison = clv_by_segment[['Historical_CLV', 'Probabilistic_CLV']].sort_values('Historical_CLV', ascending=False)\n",
    "clv_comparison.plot(kind='barh', ax=axes[0], color=['steelblue', 'coral'])\n",
    "axes[0].set_xlabel('Average CLV (£)', fontsize=12)\n",
    "axes[0].set_title('Historical vs Predicted CLV by Segment', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(['Historical CLV', 'Predicted CLV'])\n",
    "\n",
    "# All CLV methods comparison for Champions\n",
    "champions_clv = rfm[rfm['Segment'] == 'Champions'][clv_columns].mean()\n",
    "champions_clv.plot(kind='bar', ax=axes[1], color='darkgreen')\n",
    "axes[1].set_xlabel('CLV Method', fontsize=12)\n",
    "axes[1].set_ylabel('Average CLV (£)', fontsize=12)\n",
    "axes[1].set_title('Champions: CLV by Different Methods', fontsize=14, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cohort Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Determine customer cohort (first purchase month)\n",
    "customer_cohort = df_clean.groupby('CustomerID')['InvoiceDate'].min().reset_index()\n",
    "customer_cohort.columns = ['CustomerID', 'CohortDate']\n",
    "customer_cohort['CohortMonth'] = customer_cohort['CohortDate'].dt.to_period('M')\n",
    "\n",
    "# Merge with transaction data\n",
    "df_cohort = df_clean.merge(customer_cohort[['CustomerID', 'CohortMonth']], on='CustomerID')\n",
    "\n",
    "# Calculate cohort index (months since first purchase)\n",
    "df_cohort['CohortIndex'] = (df_cohort['YearMonth'] - df_cohort['CohortMonth']).apply(lambda x: x.n)\n",
    "\n",
    "# Cohort analysis: retention\n",
    "cohort_data = df_cohort.groupby(['CohortMonth', 'CohortIndex'])['CustomerID'].nunique().reset_index()\n",
    "cohort_pivot = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "# Calculate retention rates\n",
    "cohort_size = cohort_pivot.iloc[:, 0]\n",
    "retention_matrix = cohort_pivot.divide(cohort_size, axis=0) * 100\n",
    "\n",
    "print(\"Cohort Retention Matrix (first 6 months):\")\n",
    "print(retention_matrix.iloc[:, :7].round(2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize cohort retention\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(retention_matrix.iloc[:, :12], annot=True, fmt='.0f', cmap='YlGnBu', \n",
    "            cbar_kws={'label': 'Retention %'})\n",
    "plt.title('Cohort Retention Analysis (First 12 Months)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Cohort Index (Months Since First Purchase)', fontsize=12)\n",
    "plt.ylabel('Cohort Month', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Churn Risk Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Calculate churn indicators\n",
    "avg_recency = rfm['Recency'].mean()\n",
    "avg_frequency = rfm['Frequency'].mean()\n",
    "\n",
    "# Churn risk score (0-10)\n",
    "# Higher score = higher churn risk\n",
    "rfm['Churn_Risk_Score'] = (\n",
    "    (rfm['Recency'] / rfm['Recency'].max() * 4) +  # Recency impact (40%)\n",
    "    ((1 - rfm['Frequency'] / rfm['Frequency'].max()) * 3) +  # Frequency impact (30%)\n",
    "    ((1 - rfm['Monetary'] / rfm['Monetary'].max()) * 2) +  # Monetary impact (20%)\n",
    "    ((1 - rfm['Prob_Alive']) * 1)  # Probability alive (10%)\n",
    ")\n",
    "\n",
    "# Normalize to 0-10\n",
    "rfm['Churn_Risk_Score'] = (rfm['Churn_Risk_Score'] / rfm['Churn_Risk_Score'].max() * 10).round(2)\n",
    "\n",
    "# Categorize churn risk\n",
    "rfm['Churn_Risk_Category'] = pd.cut(rfm['Churn_Risk_Score'], \n",
    "                                     bins=[0, 3, 6, 10], \n",
    "                                     labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "print(\"Churn Risk Distribution:\")\n",
    "print(rfm['Churn_Risk_Category'].value_counts())\n",
    "\n",
    "print(\"\\nChurn Risk by Segment:\")\n",
    "print(rfm.groupby('Segment')['Churn_Risk_Score'].mean().sort_values(ascending=False).round(2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize churn risk\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Churn risk distribution\n",
    "rfm['Churn_Risk_Category'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'orange', 'red'])\n",
    "axes[0].set_xlabel('Churn Risk Category', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Customers', fontsize=12)\n",
    "axes[0].set_title('Churn Risk Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Churn risk by segment\n",
    "churn_by_segment = rfm.groupby('Segment')['Churn_Risk_Score'].mean().sort_values(ascending=False)\n",
    "colors_risk = ['red' if x > 6 else 'orange' if x > 3 else 'green' for x in churn_by_segment.values]\n",
    "churn_by_segment.plot(kind='barh', ax=axes[1], color=colors_risk)\n",
    "axes[1].set_xlabel('Average Churn Risk Score', fontsize=12)\n",
    "axes[1].set_title('Average Churn Risk by Segment', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(6, color='red', linestyle='--', alpha=0.5, label='High Risk Threshold')\n",
    "axes[1].axvline(3, color='orange', linestyle='--', alpha=0.5, label='Medium Risk Threshold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. High-Value At-Risk Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Identify high-CLV customers at risk of churning\n",
    "high_value_threshold = rfm['Probabilistic_CLV'].quantile(0.75)\n",
    "high_risk_threshold = 6\n",
    "\n",
    "high_value_at_risk = rfm[\n",
    "    (rfm['Probabilistic_CLV'] >= high_value_threshold) & \n",
    "    (rfm['Churn_Risk_Score'] >= high_risk_threshold)\n",
    "].sort_values('Probabilistic_CLV', ascending=False)\n",
    "\n",
    "print(f\"High-Value At-Risk Customers: {len(high_value_at_risk)}\")\n",
    "print(f\"Total CLV at Risk: £{high_value_at_risk['Probabilistic_CLV'].sum():,.2f}\")\n",
    "print(f\"Average CLV at Risk: £{high_value_at_risk['Probabilistic_CLV'].mean():,.2f}\")\n",
    "\n",
    "print(\"\\nTop 20 High-Value At-Risk Customers:\")\n",
    "high_value_at_risk[['CustomerID', 'Segment', 'Recency', 'Frequency', 'Monetary', \n",
    "                    'Probabilistic_CLV', 'Churn_Risk_Score']].head(20)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scatter plot: CLV vs Churn Risk\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Color by segment\n",
    "segments = rfm['Segment'].unique()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(segments)))\n",
    "segment_colors = {seg: colors[i] for i, seg in enumerate(segments)}\n",
    "\n",
    "for segment in segments:\n",
    "    seg_data = rfm[rfm['Segment'] == segment]\n",
    "    plt.scatter(seg_data['Churn_Risk_Score'], seg_data['Probabilistic_CLV'], \n",
    "               label=segment, alpha=0.6, s=50, c=[segment_colors[segment]])\n",
    "\n",
    "# Highlight high-value at-risk zone\n",
    "plt.axhline(high_value_threshold, color='red', linestyle='--', alpha=0.5, label='High CLV Threshold')\n",
    "plt.axvline(high_risk_threshold, color='orange', linestyle='--', alpha=0.5, label='High Risk Threshold')\n",
    "\n",
    "# Annotate the danger zone\n",
    "plt.text(7.5, rfm['Probabilistic_CLV'].max() * 0.9, 'HIGH VALUE\\nAT RISK', \n",
    "         fontsize=14, fontweight='bold', color='darkred', alpha=0.7,\n",
    "         bbox=dict(boxstyle='round', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "plt.xlabel('Churn Risk Score', fontsize=12)\n",
    "plt.ylabel('Predicted CLV (£)', fontsize=12)\n",
    "plt.title('Customer Lifetime Value vs Churn Risk', fontsize=14, fontweight='bold')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Strategic Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Comprehensive business metrics\n",
    "total_customers = len(rfm)\n",
    "total_clv = rfm['Probabilistic_CLV'].sum()\n",
    "avg_clv = rfm['Probabilistic_CLV'].mean()\n",
    "\n",
    "high_risk_customers = len(rfm[rfm['Churn_Risk_Category'] == 'High Risk'])\n",
    "high_risk_clv = rfm[rfm['Churn_Risk_Category'] == 'High Risk']['Probabilistic_CLV'].sum()\n",
    "\n",
    "champions = rfm[rfm['Segment'] == 'Champions']\n",
    "champions_clv = champions['Probabilistic_CLV'].sum()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STRATEGIC BUSINESS RECOMMENDATIONS - CLV FOCUSED\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n1. OVERALL PORTFOLIO VALUE\")\n",
    "print(f\"   Total Customers: {total_customers:,}\")\n",
    "print(f\"   Total Predicted CLV: £{total_clv:,.2f}\")\n",
    "print(f\"   Average CLV per Customer: £{avg_clv:,.2f}\")\n",
    "\n",
    "print(f\"\\n2. CHAMPIONS (Highest Value)\")\n",
    "print(f\"   Count: {len(champions):,} ({len(champions)/total_customers*100:.1f}%)\")\n",
    "print(f\"   Total CLV: £{champions_clv:,.2f} ({champions_clv/total_clv*100:.1f}%)\")\n",
    "print(f\"   Average CLV: £{champions['Probabilistic_CLV'].mean():,.2f}\")\n",
    "print(f\"   → ACTION: VIP program, early access to products, dedicated support\")\n",
    "\n",
    "print(f\"\\n3. HIGH-VALUE AT-RISK CUSTOMERS (Priority Intervention)\")\n",
    "print(f\"   Count: {len(high_value_at_risk):,}\")\n",
    "print(f\"   CLV at Risk: £{high_value_at_risk['Probabilistic_CLV'].sum():,.2f}\")\n",
    "print(f\"   → ACTION: Immediate re-engagement campaign, personalized offers\")\n",
    "print(f\"   → PRIORITY: Top 100 customers = £{high_value_at_risk.head(100)['Probabilistic_CLV'].sum():,.2f} potential loss\")\n",
    "\n",
    "print(f\"\\n4. CHURN RISK OVERVIEW\")\n",
    "print(f\"   High Risk: {high_risk_customers:,} customers ({high_risk_customers/total_customers*100:.1f}%)\")\n",
    "print(f\"   CLV at Risk: £{high_risk_clv:,.2f} ({high_risk_clv/total_clv*100:.1f}%)\")\n",
    "print(f\"   → ACTION: Win-back campaigns, identify common churn patterns\")\n",
    "\n",
    "print(f\"\\n5. RETENTION ECONOMICS\")\n",
    "# Assume 5% retention improvement\n",
    "retention_improvement = 0.05\n",
    "recoverable_value = high_risk_clv * retention_improvement\n",
    "print(f\"   If retention improves by 5%: +£{recoverable_value:,.2f} in CLV\")\n",
    "print(f\"   ROI Target: Spend up to £{recoverable_value * 0.3:,.2f} on retention (30% of value)\")\n",
    "\n",
    "print(f\"\\n6. SEGMENT-SPECIFIC CLV STRATEGIES\")\n",
    "for segment in rfm['Segment'].unique():\n",
    "    seg_data = rfm[rfm['Segment'] == segment]\n",
    "    seg_clv = seg_data['Probabilistic_CLV'].sum()\n",
    "    seg_avg_clv = seg_data['Probabilistic_CLV'].mean()\n",
    "    seg_churn = seg_data['Churn_Risk_Score'].mean()\n",
    "    print(f\"\\n   {segment}:\")\n",
    "    print(f\"   - Total CLV: £{seg_clv:,.2f} ({seg_clv/total_clv*100:.1f}%)\")\n",
    "    print(f\"   - Avg CLV: £{seg_avg_clv:,.2f} | Avg Churn Risk: {seg_churn:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Export comprehensive results\n",
    "rfm.to_csv('online_retail_ii_rfm_clv_analysis.csv', index=False)\n",
    "print(\"Full RFM + CLV analysis exported\")\n",
    "\n",
    "# Export high-priority customers\n",
    "high_value_at_risk.to_csv('high_value_at_risk_customers.csv', index=False)\n",
    "print(\"High-value at-risk customers exported\")\n",
    "\n",
    "# Export cohort analysis\n",
    "retention_matrix.to_csv('cohort_retention_matrix.csv')\n",
    "print(\"Cohort retention matrix exported\")"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
