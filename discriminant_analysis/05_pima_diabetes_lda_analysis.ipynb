{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Linear Discriminant Analysis on Pima Indians Diabetes Dataset\n## Health Analytics: Handling Medical Noise and Variance\n\n**Dataset Overview:**\n- 768 female patients of Pima Indian heritage\n- 8 diagnostic measurements\n- Binary classification: Diabetes diagnosis\n- Features: Pregnancies, glucose, blood pressure, insulin, BMI, diabetes pedigree, age\n\n**Focus:** Handling medical measurement noise and variance"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_diabetes\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, accuracy_score,\n    roc_auc_score, roc_curve, precision_recall_curve, f1_score\n)\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 8)\nprint('Libraries imported!')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 1. Data Loading"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "try:\n    df = pd.read_csv('diabetes.csv')\n    print('Real Pima dataset loaded!')\nexcept FileNotFoundError:\n    print('Creating synthetic Pima-like diabetes dataset...')\n    np.random.seed(42)\n    n = 768\n    \n    pregnancies = np.random.poisson(3.5, n)\n    glucose = np.random.normal(121, 30, n)\n    glucose = np.clip(glucose, 0, 200)\n    blood_pressure = np.random.normal(70, 12, n)\n    blood_pressure = np.clip(blood_pressure, 0, 125)\n    skin_thickness = np.random.normal(29, 10, n)\n    skin_thickness = np.clip(skin_thickness, 0, 100)\n    insulin = np.random.lognormal(4.3, 0.7, n)\n    bmi = np.random.normal(32, 6, n)\n    bmi = np.clip(bmi, 18, 50)\n    dpf = np.random.gamma(2, 0.25, n)\n    age = np.random.gamma(5, 6, n).astype(int)\n    age = np.clip(age, 21, 81)\n    \n    # Create outcome with dependencies and noise\n    risk = (\n        (glucose > 140) * 0.30 +\n        (bmi > 35) * 0.15 +\n        (age > 45) * 0.10 +\n        (dpf > 0.5) * 0.10 +\n        (pregnancies > 5) * 0.05 +\n        np.random.normal(0, 0.15, n)  # Medical noise\n    )\n    outcome = (risk > 0.35).astype(int)\n    \n    df = pd.DataFrame({\n        'Pregnancies': pregnancies,\n        'Glucose': glucose,\n        'BloodPressure': blood_pressure,\n        'SkinThickness': skin_thickness,\n        'Insulin': insulin,\n        'BMI': bmi,\n        'DiabetesPedigreeFunction': dpf,\n        'Age': age,\n        'Outcome': outcome\n    })\n    print('Synthetic dataset created!')\n\nprint(f'\\nDataset shape: {df.shape}')\nprint(f'\\nDiabetes prevalence: {df[\"Outcome\"].mean()*100:.1f}%')\nprint(f'\\nClass distribution:')\nprint(df['Outcome'].value_counts())"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('Dataset Information:')\nprint(df.info())\nprint('\\nStatistical Summary:')\ndisplay(df.describe())\n\n# Check for zeros (common data quality issue in Pima dataset)\nzero_cols = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']\nprint('\\nZero values (potential missing data):')\nfor col in zero_cols:\n    zero_count = (df[col] == 0).sum()\n    if zero_count > 0:\n        print(f'{col}: {zero_count} ({zero_count/len(df)*100:.1f}%)')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 2. Data Quality and Noise Analysis"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Handle zeros in medical measurements (likely missing values)\n# Replace with median by outcome group\ndf_clean = df.copy()\nfor col in zero_cols:\n    if (df[col] == 0).any():\n        for outcome in [0, 1]:\n            mask = (df['Outcome'] == outcome) & (df[col] > 0)\n            median_val = df.loc[mask, col].median()\n            mask_zero = (df_clean['Outcome'] == outcome) & (df_clean[col] == 0)\n            df_clean.loc[mask_zero, col] = median_val\n        print(f'Imputed {col}')\n\nprint('\\nData cleaning complete!')\nprint(f'Dataset after cleaning: {df_clean.shape}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Visualize distributions and outliers\nfig, axes = plt.subplots(3, 3, figsize=(18, 15))\naxes = axes.ravel()\n\nfeatures = df_clean.columns[:-1]\n\nfor idx, feature in enumerate(features):\n    for outcome in [0, 1]:\n        data = df_clean[df_clean['Outcome'] == outcome][feature]\n        axes[idx].hist(data, alpha=0.6, label=['No Diabetes', 'Diabetes'][outcome], bins=25)\n    axes[idx].set_title(f'{feature} Distribution')\n    axes[idx].set_xlabel(feature)\n    axes[idx].set_ylabel('Frequency')\n    axes[idx].legend()\n\naxes[-1].axis('off')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Outlier detection\nfig, axes = plt.subplots(2, 4, figsize=(20, 10))\naxes = axes.ravel()\n\nfor idx, feature in enumerate(features):\n    df_clean.boxplot(column=feature, by='Outcome', ax=axes[idx])\n    axes[idx].set_title(f'{feature} by Outcome')\n    axes[idx].set_xlabel('Outcome (0=No, 1=Yes)')\n    \n    # Calculate outliers\n    Q1 = df_clean[feature].quantile(0.25)\n    Q3 = df_clean[feature].quantile(0.75)\n    IQR = Q3 - Q1\n    outliers = ((df_clean[feature] < Q1 - 1.5*IQR) | (df_clean[feature] > Q3 + 1.5*IQR)).sum()\n    axes[idx].text(0.5, 0.95, f'Outliers: {outliers}', transform=axes[idx].transAxes, \n                  ha='center', va='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n\nplt.suptitle('Box Plots with Outlier Detection')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Correlation analysis\nplt.figure(figsize=(12, 10))\ncorr = df_clean.corr()\nsns.heatmap(corr, annot=True, cmap='coolwarm', center=0, square=True, fmt='.2f')\nplt.title('Feature Correlation Matrix')\nplt.tight_layout()\nplt.show()\n\nprint('\\nHighest correlations with Outcome:')\noutcome_corr = corr['Outcome'].sort_values(ascending=False)\nprint(outcome_corr)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 3. Data Preparation with Robust Scaling"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Prepare data\nX = df_clean.drop('Outcome', axis=1).values\ny = df_clean['Outcome'].values\nfeature_names = df_clean.columns[:-1].tolist()\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.25, random_state=42, stratify=y\n)\n\nprint(f'Training: {X_train.shape[0]} samples')\nprint(f'Test: {X_test.shape[0]} samples')\nprint(f'Features: {X_train.shape[1]}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Compare StandardScaler vs RobustScaler (for handling outliers)\nscaler_standard = StandardScaler()\nscaler_robust = RobustScaler()\n\nX_train_standard = scaler_standard.fit_transform(X_train)\nX_test_standard = scaler_standard.transform(X_test)\n\nX_train_robust = scaler_robust.fit_transform(X_train)\nX_test_robust = scaler_robust.transform(X_test)\n\nprint('Both scaling methods applied!')\nprint('\\nStandard Scaler: Uses mean and standard deviation')\nprint('Robust Scaler: Uses median and IQR (less sensitive to outliers)')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 4. LDA with Different Scaling Approaches"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# LDA with Standard Scaling\nlda_standard = LinearDiscriminantAnalysis()\nlda_standard.fit(X_train_standard, y_train)\ny_pred_standard = lda_standard.predict(X_test_standard)\ny_proba_standard = lda_standard.predict_proba(X_test_standard)[:, 1]\n\nprint('LDA WITH STANDARD SCALING')\nprint('='*70)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred_standard):.4f}')\nprint(f'ROC-AUC: {roc_auc_score(y_test, y_proba_standard):.4f}')\nprint(f'F1-Score: {f1_score(y_test, y_pred_standard):.4f}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# LDA with Robust Scaling\nlda_robust = LinearDiscriminantAnalysis()\nlda_robust.fit(X_train_robust, y_train)\ny_pred_robust = lda_robust.predict(X_test_robust)\ny_proba_robust = lda_robust.predict_proba(X_test_robust)[:, 1]\n\nprint('\\nLDA WITH ROBUST SCALING')\nprint('='*70)\nprint(f'Accuracy: {accuracy_score(y_test, y_pred_robust):.4f}')\nprint(f'ROC-AUC: {roc_auc_score(y_test, y_proba_robust):.4f}')\nprint(f'F1-Score: {f1_score(y_test, y_pred_robust):.4f}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Comparison\nprint('\\nSCALING METHOD COMPARISON')\nprint('='*70)\nprint(f'{'Metric':<20} {'Standard':>15} {'Robust':>15} {'Difference':>15}')\nprint('-'*70)\n\nmetrics = [\n    ('Accuracy', accuracy_score(y_test, y_pred_standard), accuracy_score(y_test, y_pred_robust)),\n    ('ROC-AUC', roc_auc_score(y_test, y_proba_standard), roc_auc_score(y_test, y_proba_robust)),\n    ('F1-Score', f1_score(y_test, y_pred_standard), f1_score(y_test, y_pred_robust))\n]\n\nfor name, std_val, rob_val in metrics:\n    diff = rob_val - std_val\n    print(f'{name:<20} {std_val:>15.4f} {rob_val:>15.4f} {diff:>+15.4f}')\n\nbetter_method = 'Robust' if roc_auc_score(y_test, y_proba_robust) > roc_auc_score(y_test, y_proba_standard) else 'Standard'\nprint(f'\\n\u2192 {better_method} scaling performs better for this dataset')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 5. Model Evaluation and Diagnostics"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Use better performing model\nif roc_auc_score(y_test, y_proba_robust) >= roc_auc_score(y_test, y_proba_standard):\n    lda_final = lda_robust\n    y_pred_final = y_pred_robust\n    y_proba_final = y_proba_robust\n    scaling_method = 'Robust'\nelse:\n    lda_final = lda_standard\n    y_pred_final = y_pred_standard\n    y_proba_final = y_proba_standard\n    scaling_method = 'Standard'\n\nprint(f'Using LDA with {scaling_method} Scaling for final analysis')\nprint('\\nClassification Report:')\nprint(classification_report(y_test, y_pred_final, target_names=['No Diabetes', 'Diabetes']))"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred_final)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\n# Raw counts\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\naxes[0].set_title('Confusion Matrix')\naxes[0].set_ylabel('True Label')\naxes[0].set_xlabel('Predicted Label')\n\n# Normalized\ncm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nsns.heatmap(cm_norm, annot=True, fmt='.2%', cmap='Blues', ax=axes[1],\n            xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\naxes[1].set_title('Normalized Confusion Matrix')\naxes[1].set_ylabel('True Label')\naxes[1].set_xlabel('Predicted Label')\n\nplt.tight_layout()\nplt.show()\n\ntn, fp, fn, tp = cm.ravel()\nprint(f'\\nDiagnostic Performance:')\nprint(f'Sensitivity (Recall): {tp/(tp+fn):.3f}')\nprint(f'Specificity: {tn/(tn+fp):.3f}')\nprint(f'Positive Predictive Value: {tp/(tp+fp):.3f}')\nprint(f'Negative Predictive Value: {tn/(tn+fn):.3f}')"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Cross-validation\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nif scaling_method == 'Robust':\n    cv_scores = cross_val_score(lda_robust, X_train_robust, y_train, cv=cv, scoring='roc_auc')\nelse:\n    cv_scores = cross_val_score(lda_standard, X_train_standard, y_train, cv=cv, scoring='roc_auc')\n\nprint('\\nCross-Validation Results (ROC-AUC):')\nprint(f'Scores: {cv_scores}')\nprint(f'Mean: {cv_scores.mean():.4f} \u00b1 {cv_scores.std():.4f}')\n\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, 6), cv_scores, 'bo-', linewidth=2, markersize=10)\nplt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'Mean: {cv_scores.mean():.4f}')\nplt.fill_between(range(1, 6), cv_scores.mean()-cv_scores.std(), \n                 cv_scores.mean()+cv_scores.std(), alpha=0.2)\nplt.xlabel('Fold')\nplt.ylabel('ROC-AUC Score')\nplt.title('5-Fold Cross-Validation Performance')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 6. Feature Importance and Medical Interpretation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Feature coefficients\ncoefs = lda_final.coef_[0]\nfeature_importance = pd.DataFrame({\n    'Feature': feature_names,\n    'Coefficient': coefs,\n    'Abs_Coefficient': np.abs(coefs)\n}).sort_values('Abs_Coefficient', ascending=False)\n\nprint('Feature Importance:')\ndisplay(feature_importance)\n\nplt.figure(figsize=(12, 8))\ncolors = ['red' if x < 0 else 'blue' for x in feature_importance['Coefficient'].values]\nplt.barh(range(len(feature_importance)), feature_importance['Coefficient'].values[::-1], \n         color=colors[::-1], alpha=0.7)\nplt.yticks(range(len(feature_importance)), feature_importance['Feature'].values[::-1])\nplt.xlabel('LDA Coefficient')\nplt.title('Feature Importance for Diabetes Prediction')\nplt.axvline(x=0, color='black', linestyle='--')\nplt.grid(True, alpha=0.3, axis='x')\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 7. ROC Analysis and Threshold Selection"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ROC and PR curves\nfpr, tpr, thresholds = roc_curve(y_test, y_proba_final)\nprecision, recall, pr_thresholds = precision_recall_curve(y_test, y_proba_final)\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\naxes[0].plot(fpr, tpr, linewidth=2, label=f'ROC (AUC={roc_auc_score(y_test, y_proba_final):.3f})')\naxes[0].plot([0, 1], [0, 1], 'k--', label='Random')\naxes[0].set_xlabel('False Positive Rate')\naxes[0].set_ylabel('True Positive Rate')\naxes[0].set_title('ROC Curve')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(recall, precision, linewidth=2)\naxes[1].set_xlabel('Recall (Sensitivity)')\naxes[1].set_ylabel('Precision (PPV)')\naxes[1].set_title('Precision-Recall Curve')\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 8. Comparison with QDA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Train QDA\nif scaling_method == 'Robust':\n    qda = QuadraticDiscriminantAnalysis()\n    qda.fit(X_train_robust, y_train)\n    y_pred_qda = qda.predict(X_test_robust)\n    y_proba_qda = qda.predict_proba(X_test_robust)[:, 1]\nelse:\n    qda = QuadraticDiscriminantAnalysis()\n    qda.fit(X_train_standard, y_train)\n    y_pred_qda = qda.predict(X_test_standard)\n    y_proba_qda = qda.predict_proba(X_test_standard)[:, 1]\n\nprint('COMPARISON: LDA vs QDA')\nprint('='*70)\nprint(f'{'Metric':<20} {'LDA':>15} {'QDA':>15} {'Better':>15}')\nprint('-'*70)\n\ncomparisons = [\n    ('Accuracy', accuracy_score(y_test, y_pred_final), accuracy_score(y_test, y_pred_qda)),\n    ('ROC-AUC', roc_auc_score(y_test, y_proba_final), roc_auc_score(y_test, y_proba_qda)),\n    ('F1-Score', f1_score(y_test, y_pred_final), f1_score(y_test, y_pred_qda))\n]\n\nfor name, lda_val, qda_val in comparisons:\n    better = 'LDA' if lda_val > qda_val else 'QDA' if qda_val > lda_val else 'Tie'\n    print(f'{name:<20} {lda_val:>15.4f} {qda_val:>15.4f} {better:>15}')"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## 9. Key Insights and Medical Recommendations"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "print('\\n' + '='*70)\nprint('KEY INSIGHTS: DIABETES PREDICTION WITH LDA')\nprint('='*70)\n\nprint('\\n1. DATA QUALITY MATTERS:')\nprint(f'   - Handled missing values (zeros) via imputation')\nprint(f'   - {scaling_method} scaling proved better for handling outliers')\nprint('   - Medical measurements contain significant variance')\n\nprint('\\n2. PREDICTIVE PERFORMANCE:')\nprint(f'   - ROC-AUC: {roc_auc_score(y_test, y_proba_final):.3f}')\nprint(f'   - Sensitivity: {tp/(tp+fn):.2%} (catch rate for diabetes)')\nprint(f'   - Specificity: {tn/(tn+fp):.2%}')\nprint(f'   - CV Score: {cv_scores.mean():.3f} \u00b1 {cv_scores.std():.3f}')\n\nprint('\\n3. TOP DIAGNOSTIC INDICATORS:')\nfor i, row in feature_importance.head(3).iterrows():\n    print(f'   - {row[\"Feature\"]}: coefficient = {row[\"Coefficient\"]:.4f}')\n\nprint('\\n4. MODEL SELECTION:')\nlda_auc = roc_auc_score(y_test, y_proba_final)\nqda_auc = roc_auc_score(y_test, y_proba_qda)\nif lda_auc > qda_auc:\n    print('   - LDA performs better (covariance assumption holds)')\nelse:\n    print('   - QDA performs better (different covariances per class)')\nprint(f'   - Difference: {abs(lda_auc - qda_auc):.4f}')\n\nprint('\\n5. CLINICAL RECOMMENDATIONS:')\nprint('   - Use robust preprocessing for medical data')\nprint('   - Monitor top predictive features in clinical practice')\nprint('   - Adjust decision threshold based on screening vs diagnosis context')\nprint('   - Consider ensemble with other methods for production use')\n\nprint('\\n' + '='*70)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Summary\n\n### What We Learned:\n\n1. **Handling Medical Data Quality**: Addressed missing values and outliers appropriately\n2. **Robust Preprocessing**: Compared standard vs robust scaling for noise resilience\n3. **Model Evaluation**: Used appropriate metrics for medical diagnostics\n4. **Feature Interpretation**: Identified key clinical indicators\n5. **Model Comparison**: Evaluated LDA vs QDA for this specific dataset\n\n### Why Pima Diabetes is Good for LDA:\n- Real medical measurements with inherent noise\n- Missing values (zeros) teach data preprocessing\n- Moderate class imbalance (realistic for screening)\n- Well-studied benchmark for health analytics\n- Demonstrates importance of robust methods\n- Clear clinical interpretation of features\n\n### Key Takeaways:\n- Data quality preprocessing is crucial for medical ML\n- Robust scaling can improve performance with outliers\n- Cross-validation provides reliable performance estimates\n- Feature importance guides clinical decision-making\n- Balance sensitivity vs specificity based on use case\n\n### Next Steps:\n- Try other imputation methods (KNN, iterative)\n- Experiment with feature engineering\n- Ensemble LDA with tree-based models\n- Cost-sensitive learning for asymmetric errors\n- Temporal validation if follow-up data available"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}